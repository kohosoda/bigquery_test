#!/usr/bin/env python3
"""
ãƒ¡ã‚¤ãƒ³ETLå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ â†’ GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ BigQueryãƒ­ãƒ¼ãƒ‰
"""

from inspect import _void
import os
import sys
from data_generator import SampleDataGenerator
from gcs_client import GCSClient
from bigquery_client import BigQueryClient
from bigquery_schemas import BigQuerySchemas
from config import *


def main():
    """
    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†

    å‰ææ¡ä»¶ï¼š
    - BigQueryãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒTerraformã§äº‹å‰ä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨

    å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†ã®æµã‚Œï¼š
    1. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    2. GCPç’°å¢ƒã®ç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­˜åœ¨ç¢ºèªç­‰ï¼‰
    3. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    4. GCSã‹ã‚‰BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰

    Returns:
        bool: å‡¦ç†ãŒæˆåŠŸã—ãŸå ´åˆTrueã€å¤±æ•—ã—ãŸå ´åˆFalse
    """
    print("=== Data Engineering ETL Pipeline ===\n")

    # 1. ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    print("Step 1: Generating sample data...")
    generator = SampleDataGenerator()
    generator.generate_all_data()
    print("âœ… Sample data generation completed\n")

    # 2. GCPç’°å¢ƒã®ç¢ºèª
    print("Step 2: Verifying GCP environment...")
    try:
        gcs_client = GCSClient()
        bigquery_client = BigQueryClient()
        gcs_client.setup_gcs_bucket()
        bigquery_client.setup_bigquery_dataset()  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­˜åœ¨ç¢ºèªã®ã¿
        print("âœ… GCP environment verification completed\n")
    except Exception as e:
        print(f"âŒ GCP environment verification failed: {e}")
        print("Please check your GCP credentials and ensure the BigQuery dataset is created using Terraform.")
        return False

    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        gcs_uris = _upload_files_to_gcs(gcs_client)
    except Exception as e:
        print(f"âŒ GCS upload failed: {e}")
        return False

    # 4. GCSã‹ã‚‰BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    try:
        _load_data_from_gcs_to_bigquery(bigquery_client, gcs_uris)
    except Exception as e:
        print(f"âŒ BigQuery load from GCS failed: {e}")
        return False

    print("ğŸ‰ ETL Pipeline completed successfully!")
    print(f"Data is now available in BigQuery dataset: {BIGQUERY_DATASET}")
    print(f"You can start querying the data using BigQuery console or dbt.")

    return True


def _upload_files_to_gcs(gcs_client: GCSClient) -> dict:
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    print("Step 3: Uploading files to Google Cloud Storage...")

    files_to_upload = [
        ('users.csv', 'raw/users.csv'),
        ('products.csv', 'raw/products.csv'),
        ('orders.csv', 'raw/orders.csv'),
        ('order_items.csv', 'raw/order_items.csv'),
        ('access_logs.json', 'raw/access_logs.json')
    ]

    gcs_uris = {}  # GCS URIã‚’ä¿å­˜

    for local_file, gcs_path in files_to_upload:
        local_path = f"{RAW_DATA_DIR}/{local_file}"
        if os.path.exists(local_path):
            gcs_uri = gcs_client.upload_to_gcs(local_path, gcs_path)
            gcs_uris[local_file] = gcs_uri
        else:
            print(f"âš ï¸  File not found: {local_path}")

    print("âœ… Files uploaded to GCS\n")
    return gcs_uris


def _load_data_from_gcs_to_bigquery(bigquery_client: BigQueryClient, gcs_uris: dict) -> None:
    """GCSã‹ã‚‰BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    print("Step 4: Loading data from GCS to BigQuery...")

    schemas = BigQuerySchemas.get_schemas()

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆGCSçµŒç”±ï¼‰
    csv_tables = [
        ('users.csv', 'users'),
        ('products.csv', 'products'),
        ('orders.csv', 'orders'),
        ('order_items.csv', 'order_items')
    ]

    for csv_file, table_name in csv_tables:
        if csv_file in gcs_uris:
            bigquery_client.load_csv_from_gcs_to_bigquery(
                gcs_uris[csv_file],
                table_name,
                schema=schemas.get(table_name)
            )
        else:
            print(f"âš ï¸  GCS URI not found for: {csv_file}")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆGCSçµŒç”±ï¼‰
    json_file = 'access_logs.json'
    if json_file in gcs_uris:
        bigquery_client.load_json_from_gcs_to_bigquery(
            gcs_uris[json_file],
            'access_logs',
            schema=schemas.get('access_logs')
        )
    else:
        print(f"âš ï¸  GCS URI not found for: {json_file}")

    print("âœ… Data loaded from GCS to BigQuery\n")


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
