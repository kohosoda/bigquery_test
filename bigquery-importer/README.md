# BigQuery Importer ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯ã€BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã‚’è¡Œã†Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
bigquery-importer/
â”œâ”€â”€ Dockerfile          # BigQueryã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã‚³ãƒ³ãƒ†ãƒŠã®è¨­å®š
â”œâ”€â”€ requirements.txt    # Pythonä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”œâ”€â”€ config.py          # è¨­å®šç®¡ç†
â”œâ”€â”€ data_generator.py  # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
â”œâ”€â”€ bigquery_client.py # BigQueryæ“ä½œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ gcs_client.py      # GCSæ“ä½œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ bigquery_schemas.py # BigQueryã‚¹ã‚­ãƒ¼ãƒå®šç¾©
â”œâ”€â”€ main.py           # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†
â”œâ”€â”€ upload_only.py    # GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â””â”€â”€ README.md         # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸ¯ BigQuery Importerã®å½¹å‰²

**BigQuery Importer**ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryã«åŠ¹ç‡çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ï¼š
- **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
- **GCS ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- **BigQuery ãƒ­ãƒ¼ãƒ‰**: GCSã‹ã‚‰BigQueryã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

## ğŸ“‹ å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°èª¬æ˜

### 1. `config.py` - è¨­å®šç®¡ç†
```python
# è¨­å®šã‚’ä¸€å…ƒç®¡ç†
GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID', 'your-project-id')
GCS_BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'your-bucket-name')
NUM_USERS = 1000  # ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
```

**å½¹å‰²**:
- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
- ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç®¡ç†
- GCPæ¥ç¶šæƒ…å ±ã®ä¸€å…ƒç®¡ç†

### 2. `data_generator.py` - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
```python
class SampleDataGenerator:
    def generate_users(self):      # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    def generate_products(self):   # å•†å“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    def generate_orders(self):     # æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    def generate_access_logs(self): # ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ç”Ÿæˆ
```

**å½¹å‰²**:
- **ãƒªã‚¢ãƒ«ãªECã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ**
- Fakerãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ä½œæˆ
- CSVã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›

**ç”Ÿæˆãƒ‡ãƒ¼ã‚¿**:
- **users.csv**: 1,000äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ï¼ˆåå‰ã€å¹´é½¢ã€ä½æ‰€ç­‰ï¼‰
- **products.csv**: 100å•†å“ã®æƒ…å ±ï¼ˆå•†å“åã€ä¾¡æ ¼ã€ã‚«ãƒ†ã‚´ãƒªç­‰ï¼‰
- **orders.csv**: 5,000ä»¶ã®æ³¨æ–‡æƒ…å ±
- **order_items.csv**: æ³¨æ–‡è©³ç´°ï¼ˆç´„15,000ä»¶ï¼‰
- **access_logs.json**: 10,000ä»¶ã®Webã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°

### 3. `gcp_client.py` - GCPæ“ä½œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
```python
class GCPClient:
    def setup_gcs_bucket(self):        # GCSãƒã‚±ãƒƒãƒˆä½œæˆ
    def setup_bigquery_dataset(self):  # BigQueryãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    def upload_to_gcs(self):          # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    def upload_csv_to_bigquery(self):  # CSVãƒ­ãƒ¼ãƒ‰
    def upload_json_to_bigquery(self): # JSONãƒ­ãƒ¼ãƒ‰
```

**å½¹å‰²**:
- **Google Cloud Storage**: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
- **BigQuery**: ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã¸ã®ãƒ­ãƒ¼ãƒ‰
- **è‡ªå‹•ã‚¤ãƒ³ãƒ•ãƒ©ä½œæˆ**: ãƒã‚±ãƒƒãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ä½œæˆ
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªä¾‹å¤–å‡¦ç†

### 4. `main.py` - ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†
```python
def main():
    # Step 1: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    # Step 2: GCPç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—  
    # Step 3: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®GCSã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    # Step 4: BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    # Step 5: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
```

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
1. **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**: ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
2. **GCPæº–å‚™**: ãƒã‚±ãƒƒãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªå‹•ä½œæˆ
3. **ãƒ•ã‚¡ã‚¤ãƒ«è»¢é€**: GCSã«ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
4. **ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰**: BigQueryã«ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
5. **æ¤œè¨¼**: æ­£ã—ããƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯

## ğŸš€ å®Ÿè¡Œæ–¹æ³•

### åŸºæœ¬çš„ãªå®Ÿè¡Œ
```bash
# BigQueryã‚¤ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œ
docker compose exec bigquery-importer python main.py
```

### å€‹åˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
```bash
# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®ã¿
docker compose exec bigquery-importer python -c "
from data_generator import SampleDataGenerator
generator = SampleDataGenerator()
generator.generate_all_data()
"

# GCPæ¥ç¶šãƒ†ã‚¹ãƒˆ
docker compose exec bigquery-importer python -c "
from bigquery_client import BigQueryClient
from gcs_client import GCSClient
bigquery_client = BigQueryClient()
gcs_client = GCSClient()
gcs_client.setup_gcs_bucket()
bigquery_client.setup_bigquery_dataset()
"
```

## ğŸ“Š å®Ÿè¡Œçµæœ

### æˆåŠŸæ™‚ã®å‡ºåŠ›ä¾‹
```
=== BigQuery Data Import Pipeline ===

Step 1: Generating sample data...
âœ… Sample data generation completed

Step 2: Setting up GCP environment...
Creating new bucket: your-bucket-name
Creating new dataset: ecommerce_data
âœ… GCP environment setup completed

Step 3: Uploading files to Google Cloud Storage...
âœ… Files uploaded to GCS

Step 4: Loading data to BigQuery...
Loaded 1000 rows to project.ecommerce_data.users
Loaded 100 rows to project.ecommerce_data.products
Loaded 5000 rows to project.ecommerce_data.orders
Loaded 14930 rows to project.ecommerce_data.order_items
Loaded 10000 rows to project.ecommerce_data.access_logs
âœ… Data loaded to BigQuery

Step 5: Validating loaded data...
âœ… Data validation completed

ğŸ‰ BigQuery Import Pipeline completed successfully!
```

## ğŸ”§ æŠ€è¡“çš„ãªãƒã‚¤ãƒ³ãƒˆ

### ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ
- **æ­£è¦åŒ–**: users, products, ordersã®é–¢ä¿‚ã‚’é©åˆ‡ã«è¨­è¨ˆ
- **ãƒªã‚¢ãƒ«ãªãƒ‡ãƒ¼ã‚¿**: Fakerã§å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- **ã‚¹ã‚­ãƒ¼ãƒå®šç¾©**: BigQueryã®å‹ã«æœ€é©åŒ–

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
try:
    gcp_client.setup_gcs_bucket()
    print("âœ… GCS setup completed")
except Exception as e:
    print(f"âŒ GCS setup failed: {e}")
    return False
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- **ãƒãƒƒãƒå‡¦ç†**: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå‡¦ç†
- **ã‚¹ã‚­ãƒ¼ãƒæŒ‡å®š**: BigQueryãƒ­ãƒ¼ãƒ‰æ™‚ã®å‹æŒ‡å®šã§é«˜é€ŸåŒ–
- **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®åŒæ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

## ğŸ“ å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ

### ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°åŸºç¤
1. **ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­è¨ˆ**: ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’è¨­è¨ˆ
2. **ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†**: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
3. **ã‚¯ãƒ©ã‚¦ãƒ‰é€£æº**: GCP APIã®æ´»ç”¨

### PythonæŠ€è¡“
1. **ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘**: ã‚¯ãƒ©ã‚¹è¨­è¨ˆã¨ãƒ¡ã‚½ãƒƒãƒ‰åˆ†å‰²
2. **å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: pandas, google-cloud-*ã®æ´»ç”¨
3. **è¨­å®šç®¡ç†**: ç’°å¢ƒå¤‰æ•°ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†

### å®Ÿå‹™ã‚¹ã‚­ãƒ«
1. **ã‚¤ãƒ³ãƒ•ãƒ©è‡ªå‹•åŒ–**: å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã®è‡ªå‹•ä½œæˆ
2. **ãƒ­ã‚°å‡ºåŠ›**: å‡¦ç†çŠ¶æ³ã®å¯è¦–åŒ–
3. **ã‚¨ãƒ©ãƒ¼å¯¾å¿œ**: å•é¡Œã®ç‰¹å®šã¨è§£æ±º

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

**1. GCPèªè¨¼ã‚¨ãƒ©ãƒ¼**
```bash
# è§£æ±ºæ–¹æ³•
gcloud auth login
gcloud auth application-default login
```

**2. æ¨©é™ã‚¨ãƒ©ãƒ¼**
- BigQuery Admin ã¾ãŸã¯ BigQuery User ãƒ­ãƒ¼ãƒ«ãŒå¿…è¦
- Storage Admin ã¾ãŸã¯ Storage Object Admin ãƒ­ãƒ¼ãƒ«ãŒå¿…è¦

**3. CSVå½¢å¼ã‚¨ãƒ©ãƒ¼**
- data_generator.pyã§ã‚«ãƒ©ãƒ é †åºã‚’ç¢ºèª
- BigQueryã‚¹ã‚­ãƒ¼ãƒã¨ã®ä¸€è‡´ã‚’ç¢ºèª

**4. ç’°å¢ƒå¤‰æ•°æœªè¨­å®š**
```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
cat .env

# ã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•
docker compose down && docker compose up -d
```

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

BigQueryã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã—ãŸã‚‰ã€æ¬¡ã¯dbtã§ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«é€²ã¿ã¾ã™ï¼š
1. **dbtãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ**: ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°â†’ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆå¤‰æ›
2. **ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆ**: dbtãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

ã“ã®BigQueryã‚¤ãƒ³ãƒãƒ¼ãƒˆå‡¦ç†ã«ã‚ˆã‚Šã€åˆ†æå¯èƒ½ãªçŠ¶æ…‹ã§ãƒ‡ãƒ¼ã‚¿ãŒBigQueryã«æ ¼ç´ã•ã‚Œã€æœ¬æ ¼çš„ãªãƒ‡ãƒ¼ã‚¿åˆ†æã®åŸºç›¤ãŒå®Œæˆã—ã¾ã™ã€‚
